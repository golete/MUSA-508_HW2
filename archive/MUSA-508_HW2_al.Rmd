---
title: "MUSA 508, Hedonic Home Price Prediction"
author: "Ericson, E. & León, A."
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, error = F, message = F, results = F)

# load libraries
# TODO: check if using gridExtra, jtools, ggstance
library(tidyverse)
library(tidycensus)
library(sf)
library(spdep)
library(caret)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(mapview)
library(stargazer)
library(vtable)

# load book functions
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

# set Census API key
census_api_key("e79f3706b6d61249968c6ce88794f6f556e5bf3d", overwrite = FALSE)

# block scientific notation
options(scipen = 999)

# set shortcuts
g <- glimpse
m <- mapview
palette5 <- c("#f0f9e8","#bae4bc","#7bccc4","#139ed1","#0868ac")

# set map styling options
mapTheme <- function() {
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(face = "italic"),
    plot.caption = element_text(hjust = 0)
  )
}

# set plot styling options
plotTheme <- function() {
  theme(
    axis.ticks = element_blank(),
#    axis.title = element_blank(),
    legend.title = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line(color = "gray75", size = 0.1),
    panel.grid.minor = element_line(color = "gray75", size = 0.1),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(face = "italic"),
    plot.caption = element_text(hjust = 0))
}

# Set geography and period of time analyzed
year <- 2019
state <- 08
county <- 13

```

#### 1. Introduction





The Zestimate, [Zillow’s algorithm for predicting home values](https://www.zillow.com/how-much-is-my-home-worth/), has increasingly become the predominant reference for buying and selling houses in the real estate market, often being mistaken for an appraisal tool. Consequently, inaccuracies in their model predictions can, now more than ever, [critically affect homeowner’s personal equity](https://www.washingtonpost.com/realestate/zillow-faces-lawsuit-over-zestimate-tool-that-calculates-a-houses-worth/2017/05/09/b22d0318-3410-11e7-b4ee-434b6d506b37_story.html) and undermine reliability in the tool.

Our goal was to develop a Machine Learning pipeline for predicting house prices that is enhanced with additional layers of data, including local features specific to Boulder, Colorado, while also being tested and adjusted for spatial and socio-economic biases to ensure its generalizability, and thus giving it more predictive power.

However, this task presents two important challenges. First, we are not taking into account any previous sales or tax assessment data that could better inform our model. Second, we are conducting this analysis in Boulder County in Colorado, a county that is not itself a coherent urban unit but rather is composed of Denver’s outer suburbs, medium-sized towns and almost half of its area covered in forest. Furthermore, Boulder is a very homogeneous and particular county, with a 91% white population, a median income of \$81,390 (compared to \$62,843 nation-wide) and 61% of people 25 or older with higher education (compared to 31% nation-wide) in 2019 estimates.

We implemented a Linear Regression supervised Machine Learning model for predicting house prices (explained in more detail in part 3) and built up a pipeline where we can input data (we have previously cleaned and wrangled), evaluate it, take it out or leave it depending on its usefulness, process it and test it on home samples we have set apart for predicting, while getting metrics on the current effectiveness of the model. Each version of the model was saved separately and its effectiveness logged, so it can be plugged in or out as needed. This process can be then repeated many times until the best possible is obtained.



>Briefly summarize your results.<<<<<<<<

***

#### 2. Data collection





We structured the data gathering process into five groups, according to their origin and function:

+ A. The base home features provided by the client, including the price dependent variable.

```{r data A: homes}

# --- DATA WRANGLING ---

# A. HOME VALUE DATA

# set Boulder CRS
boulderCRS <- "ESRI:102253" # NAD 1983 HARN StatePlane Colorado North FIPS 0501

# read in home value data
data <- st_read("studentData.geojson") %>%
  st_set_crs("ESRI:102254") %>%
  st_transform(boulderCRS) %>%
  filter(toPredict == 0)

# A1. Clean and recode home data

# recode missing data and engineered features
homeRecodes <- data %>%
  mutate(
    # change year to factor from float
    year = as.factor(year),
    # calculate log of price to normalize positive skew
    logPrice = log(price),
    # recode missing construction material values
    constMat = case_when(
      ConstCode == 0 ~ "Missing",
      ConstCode == 300 ~ "Unspecified",
      ConstCode > 300 ~ as.character(ConstCodeDscr)
    ),
    # recode basement as dummy
    hasBasement = if_else(bsmtType == 0, 0, 1),
    # recode car storage as garage dummy
    hasGarage = if_else(str_detect(carStorageTypeDscr, "GARAGE"), 1, 0),
    # recode a/c as dummy, excluding fans, evaporative coolers, and unspecified
    hasAC = replace_na(if_else(Ac == 210, 1, 0), 0),
    # recode missing heating values
    heatingType = case_when(
      is.na(Heating) ~ "None",
      Heating == 800 ~ "Unspecified",
      Heating > 800 ~ as.character(HeatingDscr)
    ),
    # recode missing primary exterior wall values
    extWall = if_else(ExtWallPrim == 0, "Missing", as.character(ExtWallDscrPrim)),
    # recode missing secondary exterior wall values
    extWall2 = if_else(is.na(ExtWallSec), "None", as.character(ExtWallDscrSec)),
    # recode missing interior wall values
    intWall = if_else(is.na(IntWall), "Missing", as.character(IntWallDscr)),
    # recode missing roof cover values
    roofType = if_else(is.na(Roof_Cover), "Missing", as.character(Roof_CoverDscr)),
    # recode quality as numeric variable
    qualityNum = case_when(
      qualityCode == 10 ~ 1, # QualityCodeDscr == "LOW "
      qualityCode == 20 ~ 2, # "FAIR "
      qualityCode == 30 ~ 3, # "AVERAGE "
      qualityCode == 31 ~ 4, # "AVERAGE + "
      qualityCode == 32 ~ 5, # "AVERAGE ++ "
      qualityCode == 40 ~ 6, # "GOOD "
      qualityCode == 41 ~ 7, # "GOOD + "
      qualityCode == 42 ~ 8, # "GOOD ++ "
      qualityCode == 50 ~ 9, # "VERY GOOD "
      qualityCode == 51 ~ 10, # "VERY GOOD + "
      qualityCode == 52 ~ 11, # "VERY GOOD ++ "
      qualityCode == 60 ~ 12, # "EXCELLENT "
      qualityCode == 61 ~ 13, # "EXCELLENT + "
      qualityCode == 62 ~ 14, # "EXCELLENT++ "
      qualityCode == 70 ~ 15, # "EXCEPTIONAL 1 "
      qualityCode == 80 ~ 16, # "EXCEPTIONAL 2 "
    ),
    # recode missing construction material values
    constMat = case_when(
      ConstCode == 0 ~ "Missing",
      ConstCode == 300 ~ "Unspecified",
      ConstCode > 300 ~ as.character(ConstCodeDscr)
    ),
    # recode missing primary exterior wall values
    extWall = if_else(
      is.na(ExtWallPrim) | ExtWallPrim == 0, "Missing", 
      as.character(ExtWallDscrPrim)
      ),
    # recode builtYear as builtEra
    builtEra = case_when(
      builtYear < 1910 ~ "Pre-1910",
      between(builtYear, 1910, 1919) ~ "1910s",
      between(builtYear, 1920, 1929) ~ "1920s",
      between(builtYear, 1930, 1939) ~ "1930s",
      between(builtYear, 1940, 1949) ~ "1940s",
      between(builtYear, 1950, 1959) ~ "1950s",
      between(builtYear, 1960, 1969) ~ "1960s",
      between(builtYear, 1970, 1979) ~ "1970s",
      between(builtYear, 1980, 1989) ~ "1980s",
      between(builtYear, 1990, 1999) ~ "1990s",
      between(builtYear, 2000, 2009) ~ "2000s",
      between(builtYear, 2010, 2019) ~ "2010s",
      builtYear >= 2020 ~ "2020s"
    ),
    # recode section_num as manySections
    manySections = if_else(section_num > 1, 1, 0),
    # recode design to remove all caps for table
    designType = if_else(
      designCode == "0120", "Multi-Story Townhouse", as.character(designCodeDscr)
    ),
    # calculate total rooms, including baths
    nbrRooms = nbrBedRoom+nbrRoomsNobath+nbrFullBaths+nbrThreeQtrBaths+(nbrHalfBaths/2),
    # effective age (time since last major renovation)
    # effectiveAge = if_else(year <= builtYear, 0, year - EffectiveYear), # negative values recoded as 0
  )
  

# create clean data frame for modeling
homeData <- homeRecodes %>%
  # drop extreme outliers identified as data entry errors
  filter(!MUSA_ID %in% c(8735,1397,5258)) %>%
  # drop unneeded columns
  dplyr::select(
    # same for all
    -bldgClass,
    -bldgClassDscr,
    -status_cd,
    # not needed
    -saleDate,
    -address,
    # too much missing data
    -Stories,
    -UnitCount,
    # cleaned
    -designCode,
    -qualityCode,
    -ConstCode,
    -ConstCodeDscr,
    -bsmtType,
    -bsmtTypeDscr,
    -carStorageType,
    -carStorageTypeDscr,
    -Ac,
    -AcDscr,
    -Heating,
    -HeatingDscr,
    -ExtWallPrim,
    -ExtWallDscrPrim,
    -ExtWallSec,
    -ExtWallDscrSec,
    -IntWall,
    -IntWallDscr,
    -Roof_Cover,
    -Roof_CoverDscr,
    # recoded
    -qualityCodeDscr,
    -builtYear
  )

# isolate home IDs to use in spatial joins
homeIDs <- data %>%
  dplyr::select(MUSA_ID, geometry)

```
+ B. Boundaries data, related to county limits, municipalities, neighborhoods, zoning, etc.

```{r data B: boundaries}

# B. BOUNDARY DATA (Neighborhoods, school districts, city, etc.)

# B1. Boulder county boundaries

countyLimits <- st_read('County_Boundary.geojson') %>%
  select(OBJECTID, geometry) %>%
  st_transform(boulderCRS)

# B2. Boulder city and other cities/zones boundaries

zones <- st_read('Zoning_-_Zoning_Districts.geojson') %>%
  st_transform(boulderCRS) %>%
  select(ZONEDESC, geometry) %>%
  filter(ZONEDESC != 'Boulder') %>%
  group_by(ZONEDESC) %>%
  rename(SUBCOMMUNITY = ZONEDESC) %>%
  summarize(geometry = st_union(geometry))


# B3. Boulder City Zoning Districts
districts <- st_read('Zoning_Districts.geojson') %>%
  st_transform(boulderCRS) %>%
  select(OBJECTID, ZONING, ZNDESC, geometry)

# Load the subcommunities / neighborhoods rough boundaries
subcomms <-  st_read('Subcommunities.geojson') %>%
  st_transform(boulderCRS)


# Join the region zoning polygons with the subcommunities polygons and union
cityHoods <- st_join(districts, subcomms, largest=TRUE) %>%
  select(SUBCOMMUNITY, geometry) %>%
  group_by(SUBCOMMUNITY) %>%
  summarize(geometry = st_union(geometry))


# FINAL NEIGHBORHOOD DATA TO USE
neighborhoods <- rbind(zones, cityHoods) %>%
  rename(community = SUBCOMMUNITY)

neighborhoodData <- st_join(homeIDs, neighborhoods) %>%
  distinct(.,MUSA_ID, .keep_all = TRUE) %>%
  st_drop_geometry() 


# import tract boundaries as proxy for neighborhoods
tracts <- 
  get_acs(geography = "tract",
          variables = "B19013_001E", # median household income
          year = year,
          state = state,
          county = county,
          geometry = T,
          output = "wide") %>%
  dplyr::select(GEOID, B19013_001E, geometry)%>%
  rename(tract = GEOID,
         medianIncome = B19013_001E) %>%
  st_transform(boulderCRS)

# isolate tract boundaries to join to home data
tractsData <- st_join(homeIDs, tracts) %>%
  dplyr::select(-medianIncome) %>%
  st_drop_geometry()

```
+ C. Data from the 2019 American Community Survey, including home ownership and vacancy rates, education, income and demographics on the block group level.

```{r data C: Census}

# C. CENSUS DATA

# review available variables
acsVariableList <- load_variables(year,"acs5", cache = TRUE)

# define variables to import
varsC <- c("B02001_001E", # race: total
           "B02001_002E", # race: white alone
           'B25003_001E', # tenure: occupied housing units
           'B25003_002E', # tenure: owner-occupied
           'B25002_001E', # occupancy: total housing units
           'B25002_003E', # occupancy: vacant housing units
           'B15003_001E', # educational attainment: total
           'B15003_022E', # educational attainment: bachelor's degree
           'B15003_023E', # educational attainment: master's degree
           'B15003_024E', # educational attainment: professional degree
           'B15003_025E', # educational attainment: doctorate degree
           'B19001_001E', # household income: total
           'B19001_002E', # household income: less than $10k
           'B19001_003E', # household income: $10-15k
           'B19001_004E', # household income: $15-20k
           'B19001_005E', # household income: $20-25k
           'B19001_006E', # household income: $25-30k
           'B19001_007E', # household income: $30-35k
           'B19001_008E', # household income: $35-40k
           'B19001_009E', # household income: $40-45k
           'B19001_010E', # household income: $45-50k
           'B19001_011E', # household income: $50-60k
           'B19001_012E', # household income: $60-75k
           'B19001_013E', # household income: $75-100k
           'B19001_014E', # household income: $100-125k
           'B19001_015E', # household income: $125-150k
           'B19001_016E', # household income: $150-200k
           'B19001_017E') # household income: $200 or more

# import variables from ACS 2019 5-year
blockGroups <- 
  get_acs(geography = "block group",
          variables = varsC,
          year = year,
          state = state,
          county = county,
          geometry = T,
          output = 'wide') %>%
  dplyr::select(-ends_with('M')) %>%
  rename(# white population
         raceTotal = B02001_001E, # race: total
         whiteAlone = B02001_002E, # race: white alone
         # vacant housing units
         totalUnits = B25002_001E, # occupancy status: total
         vacantUnits = B25002_003E, # occupancy status: vacant
         # homeowners
         occupiedUnits = B25003_001E, # tenure: total
         ownerOccupied = B25003_002E, # tenure: owner-occupied
         # highest educational attainment
         eduTotal = B15003_001E, # educational attainment: total
         eduBachs = B15003_022E, # educational attainment: bachelor's degree
         eduMasts = B15003_023E, # educational attainment: master's degree
         eduProfs = B15003_024E, # educational attainment: professional degree
         eduDocts = B15003_025E, # educational attainment: doctorate degree
         # household income
         incomeTotal = B19001_001E, # household income: total
         income000 = B19001_002E, # household income: less than $10k
         income010 = B19001_003E, # household income: $10-15k
         income015 = B19001_004E, # household income: $15-20k
         income020 = B19001_005E, # household income: $20-25k
         income025 = B19001_006E, # household income: $25-30k
         income030 = B19001_007E, # household income: $30-35k
         income035 = B19001_008E, # household income: $35-40k
         income040 = B19001_009E, # household income: $40-45k
         income045 = B19001_010E, # household income: $45-50k
         income050 = B19001_011E, # household income: $50-60k
         income060 = B19001_012E, # household income: $60-75k
         income075 = B19001_013E, # household income: $75-100k
         income100 = B19001_014E, # household income: $100-125k
         income125 = B19001_015E, # household income: $125-150k
         income150 = B19001_016E, # household income: $150-200k
         income200 = B19001_017E # household income: $200k or more
         )%>%
  mutate(pctWhite = whiteAlone/raceTotal,
         pctVacant = vacantUnits/totalUnits,
         pctOwnerOccupied = ownerOccupied/occupiedUnits,
         # calculate percent with bachelor's or higher
         # TODO: compare percent postgraduate?
         pctHigherEdu = if_else(
           eduTotal > 0, (eduBachs + eduMasts + eduProfs + eduDocts)/eduTotal, 0
         ),
         # calculate percent in each income category
         pctIncome000 = income000/incomeTotal,
         pctIncome010 = income010/incomeTotal,
         pctIncome015 = income015/incomeTotal,
         pctIncome020 = income020/incomeTotal,
         pctIncome025 = income025/incomeTotal,
         pctIncome030 = income030/incomeTotal,
         pctIncome035 = income035/incomeTotal,
         pctIncome040 = income040/incomeTotal,
         pctIncome045 = income045/incomeTotal,
         pctIncome050 = income050/incomeTotal,
         pctIncome060 = income060/incomeTotal,
         pctIncome075 = income075/incomeTotal,
         pctIncome100 = income100/incomeTotal,
         pctIncome125 = income125/incomeTotal,
         pctIncome150 = income150/incomeTotal,
         pctIncome200 = income200/incomeTotal,
         # recode final income features after exploratory analysis 
         pctIncomeBelow100k = (
           income000 + income010 + income015 + income020 + income025 + 
             income030 + income035 + income040 + income045 + income050 + 
             income060 + income075
           )/incomeTotal,
         pctIncomeAbove200k = pctIncome200
        ) %>%
  select(GEOID, pctWhite, pctVacant, pctOwnerOccupied, pctHigherEdu, 
         pctIncomeBelow100k, pctIncomeAbove200k, geometry) %>%
  rename(blockGroup = GEOID) %>%
  st_transform(boulderCRS)


blockGroupBoundaries <- blockGroups %>%
  select(blockGroup, geometry)

censusData <- st_join(homeIDs, blockGroupBoundaries) %>%
  st_drop_geometry() %>%
  left_join(., blockGroups, by='blockGroup') %>%
  select(-geometry, -blockGroup)


```
+ D. Other open data, including FEMA, municipal services or amenities and crime data, if pertinent.

```{r data D: other data}

# D. OTHER DATA (CRIME, FEMA, etc.)

# D1. Wildfire history data

wildfires <-
  st_read('Wildfire_History.geojson') %>%
  filter(ENDDATE > "2001-10-19 00:00:00") %>% # FILTER to only fires that happened after 2000
  select(NAME, geometry) %>%
  st_transform(boulderCRS) %>%
  st_buffer(1610) %>%
  st_union() %>%
  st_sf() %>%
  mutate(wildfireHazard = 1)

wildfireData <- st_join(homeIDs, wildfires) %>%
  st_drop_geometry() %>%
  mutate(wildfireHazard = replace_na(wildfireHazard, 0))


# D2. CHAMP floodplain maps

fldrisk = c(AE = 4, AH = 3, AO = 2, X = 1)

floodplains <- 
  st_read('Floodplain_-_BC_Regulated.geojson') %>%
  st_transform(boulderCRS) %>%
  select(SFHA_TF, #Special Flood Hazard Area. If the area is within the SFHA.
         FLD_ZONE, #Flood Zone. This is a flood zone designation.
         geometry) %>%
  mutate(FLD_ZONE = recode(FLD_ZONE, !!!fldrisk, .default = 0)) %>%
  group_by(FLD_ZONE) %>%
  summarize(geometry = st_union(geometry))%>%
  rename(floodRisk = FLD_ZONE)

floodData <- st_join(homeIDs, floodplains) %>%
  st_drop_geometry() %>%
  mutate(floodRisk = replace_na(floodRisk, 0))

```
+ E. “Experimental” data, meaning variables like location of marijuana dispensaries or distance to major highways, that are not conventionally associated with house prices.

```{r data E: experimental}

# E1. Whole Foods locations

wholeFoodsLocations <- st_read("wholefoodsmarkets_boulderCO.csv")

wholeFoods <- st_as_sf(wholeFoodsLocations, coords = c("lon", "lat"), crs = 4326) %>% #4326
  dplyr::select(-phone, -address) %>%
  st_buffer(4023) %>%
  st_union() %>%
  st_sf() %>%
  st_transform(boulderCRS)

wholeFoodsBuffer <- st_join(homeIDs, wholeFoods, left = FALSE) 

wholeFoodsData <- homeIDs %>%
  mutate(wholeFoods = ifelse(MUSA_ID %in% wholeFoodsBuffer$MUSA_ID, 1, 0))  %>%
  st_drop_geometry()


# E2. Marijuana dispensaries

marijuana <- st_read("Marijuana_Establishments.geojson") %>%
  dplyr::select(OBJECTID, Type, geometry) %>%
  st_buffer(1609) %>%
  st_union() %>%
  st_sf() %>%
  st_transform(boulderCRS)

marijuanaBuffer <- st_join(homeIDs, marijuana, left = FALSE) 

marijuanaData <- homeIDs %>%
  mutate(marijuana = ifelse(MUSA_ID %in% marijuanaBuffer$MUSA_ID, 1, 0)) %>%
  st_drop_geometry()


```

Given the richness of data from A and C, the data wrangling process focused on selecting **what data is relevant** for predicting house prices and which new variables could be produced from this raw data, and the results were ready to use. On the other hand, B and D were conditioned by **what data is available** and involved a more intensive understanding of the data, determining parameters on how to measure its possible effects on home prices. Data in the E group was treated more akin to that of B and D, but with a looser expectation of what its effects could be.


```{r data F: combined data}

# F. Combine data sets

finalData <-
  left_join(homeData, neighborhoodData, by = 'MUSA_ID') %>%
  left_join(., censusData, by = 'MUSA_ID') %>%
  left_join(., wildfireData, by = 'MUSA_ID') %>%
  left_join(., floodData, by = 'MUSA_ID') %>%
  left_join(., wholeFoodsData, by = 'MUSA_ID') %>%
  left_join(., marijuanaData, by = 'MUSA_ID') %>%
  left_join(., tractsData, by = 'MUSA_ID')

```



```{r EDA: Initial analysis}

# 2. EXPLORATORY ANALYSIS

# Decide the geometry to use for neighborhoods
# 1 for neighborhoods and 2 for census tracts

idx <- 2

geounit <- c('community', 'tract')

finalData <- finalData %>%
  rename(neighborhood = geounit[idx]) %>%  # geometry to be used
  select(-geounit[-idx])                   # geometry not to be used

```


```{r EDA: Correlations}

# plot correlation of individual variables with home values

# select numeric variables for correlation matrix
numericVars <- select_if(st_drop_geometry(finalData), is.numeric) %>%
  dplyr::select(
    # omit for more legible chart
    -toPredict,
    -MUSA_ID) %>%
  na.omit()

# create numeric variable correlation matrix and convert to data frame
corMatrix <- cor(numericVars)
corDF <- as.data.frame(as.table(corMatrix)) %>%
  rename(Cor = Freq)

# review numeric variables most correlated with price
corPrice <- filter(corDF, Var1 == "price")
corLogPrice <- filter(corDF, Var1 == "logPrice") # stronger correlations

corFinishedSF <- filter(corDF, Var1 == "TotalFinishedSF")



# select non-numeric variables to convert to dummies
nonNumericVars <- select_if(st_drop_geometry(finalData), Negate(is.numeric)) %>%
  na.omit()

# convert categorical variables to dummies
dummyVars <- dummy.data.frame(nonNumericVars)

# add log(price) column to dummies
dummiesWithLogPrice <- cbind(finalData$logPrice, dummyVars)

# create dummy variable correlation matrix as data frame
dummyCorDF <- as.data.frame(as.table(cor(dummiesWithLogPrice)))

# review dummy variables most correlated with price
dummyCorLogPrice <- filter(dummyCorDF, Var1 == "finalData$logPrice")%>%
  rename(Cor = Freq)

hoodDummyVars <- dummyVars %>%
  select(starts_with('neighborhood'))

# combine numeric and dummy variables
allVars <- cbind(numericVars, dummyVars)





```



>Present a table of summary statistics with variable descriptions.

##### Summary statistics with `stargazer`:

```{r}

summaryStats <- sumtable(as.data.frame(finalData), add.median = T, out = "return")

summaryStats %>%
  kbl(
    caption = "Summary Statistics", 
    digits = 2, 
    format.args = list(big.mark = ","),
    knitr.kable.NA = "*"
  ) %>%
  kable_classic()

```


##### Correlation matrix of numeric variables:


```{r correlation matrix}

# select numeric variables
numericVars <- select_if(st_drop_geometry(finalData), is.numeric) %>%
  dplyr::select(
    # omit for more legible chart
    -toPredict,
    -MUSA_ID) %>%
  na.omit()

# generate correlation matrix
ggcorrplot(
   round(cor(numericVars), 1),
   p.mat = cor_pmat(numericVars),
   show.diag = TRUE,
   colors = c("#25cb10", "#ffffff", "#fa7800"),
   type = "lower",
   insig = "blank"
) +
   labs(title = "Correlation across numeric variables") +
  plotTheme()


```


##### Interesting correlations:

```{r correlation scatterplots}

st_drop_geometry(finalData) %>%
  dplyr::select(price, pctHigherEdu, pctIncomeAbove200k, pctIncomeBelow100k, pctVacant) %>%
  pivot_longer(cols = !price, names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(Value, price)) +
    geom_point(size = 0.5) +
    geom_smooth(method = "lm", color = "#FA7800") +
    facet_wrap(~Variable, ncol = 2, scales = "free") +
    labs(title = "Price as a function of continuous variables") +
  plotTheme()
```


##### Mapping prices:

To visualize the distribution of prices we first look at the mean price by 'Neighborhood'.

``` {r spatial distribution of sale prices}

# --- 1. sale price by Census tract ---

# import county limits
countyLimits <- st_read('County_Boundary.geojson') %>%
  select(OBJECTID, geometry) %>%
  st_transform(boulderCRS)

# calculate mean sale price by tract
priceByTract <- finalData %>%
  dplyr::select(tract, price) %>%
  st_drop_geometry() %>%
  group_by(tract) %>%
  summarize(
    meanPrice = mean(price),
    medianPrice = median(price)
  ) %>%
  left_join(., tracts) %>%
  dplyr::select(-medianIncome) %>%
  st_sf()

ggplot() +
  geom_sf(
    data = priceByTract, 
    aes(fill = meanPrice),
    lwd = 0.1,
    color = "black",
    alpha = 0.7
  ) +
  scale_fill_viridis_c("Mean sale price",
                       direction = -1,
                       option = "G") +
  geom_sf(data = countyLimits, fill = "transparent") +
  labs(
    title = "Spatial distribution of home prices",
    subtitle = "Mean sale price by Census tract",
    caption = "Fig. X.X" # TODO: number figure before finalizing
  ) +
  mapTheme()

```


And we can also look at the distribution of house prices between Municipalities and unincorporated urbanized areas:


```{r Individual home sale prices}

# --- 2. individual home sale prices ---

# import municipality limits
munis <- st_read('Municipalities.geojson') %>%
  select(ZONEDESC, geometry) %>%
  st_transform(boulderCRS)

# map individual home prices
ggplot() +
  geom_sf(
    data = homeData,
    size = 0.5,
    aes(color = price)
  ) +
  scale_color_viridis_c("Sale price",
                       direction = -1, 
                       option = "D") +
  geom_sf(data = countyLimits, fill = "transparent") +
  geom_sf(data = munis, fill = "transparent", lwd = 0.5) +
  labs(
    title = "Spatial distribution of sale prices",
    subtitle = "Individual homes relative to county and municipality boundaries",
    caption = "Fig. X.X" # TODO: number figure before finalizing
  ) +
  mapTheme()

```



> Develop 3 maps of 3 of your most interesting independent variables.











> Include any other maps/graphs/charts you think might be of interest.




***

### 3. Methods





Developing and refining a Machine Learning model is not a linear process but rather a cyclical one. Nevertheless, there is a series of continuous steps that are laid out to test each version of the model.

First, we look into the correspondence (or _correlation_) between the sample home prices and each of the input (or _dependent_) variables. We visualize this as individual scatter plots or as an overall matrix diagram that summarizes correlations among all variables, so we can get an initial sense of which data is useful and which is too redundant.

Second, we perform a linear regression: a method for determining how much home prices change by each additional unit of the input variables, informing us of not only their individual effectiveness but also of their aggregate power. This way we can further evaluate which data should make it into our model.



```{r OLS regression and modeling}

# 3. LINEAR  and variable exclusion

finalDataReg <- lm(logPrice ~ .,
                 data = st_drop_geometry(finalData) %>%
                   dplyr::select(-price, -MUSA_ID, -toPredict))

# 4. MODEL ESTIMATION & VALIDATION

# model 2
d2n <- finalData %>%
  dplyr::select(MUSA_ID,
                price,
                logPrice,
                nbrRooms,
                qualityNum,
                builtEra,
                # basementDummy,
                # garageDummy,
                # designCodeDscr,
                extWall,
                starts_with('neigh'),
                starts_with('pctIncome'),
                pctVacant,
                pctHigherEdu,
                floodRisk,
                wholeFoods,
                toPredict,
                geometry)

# model 3
d3n <- finalData %>%
  dplyr::select(MUSA_ID,
                price,
                logPrice,
                nbrRooms,
                qualityNum,
                builtEra,
                # basementDummy,
                # garageDummy,
                # designCodeDscr,
                extWall,
                starts_with('neigh'),
                starts_with('pctIncome'),
                pctVacant,
                pctHigherEdu,
                floodRisk,
                wholeFoods,
                toPredict,
                geometry) 


RegA <- lm(logPrice ~ .,
           data = (st_drop_geometry(d2n)) %>%
             dplyr::select(-price, -MUSA_ID, -toPredict))


```


The next step, cross validation, divides the data into two groups: a ‘training’ set that establishes the coefficients between variables and known house prices, and a ‘test’ set on which to predict house prices using these coefficients. By comparing the predicted values with the real ones, we measure the amount of error in our model, meaning **_how wrong_** our predictions are, or alternatively, **_how precisely_** can our model predict. At this point we also check for _outliers_ that may have a significantly extreme error in price and, using other tools like Google Maps, we assess if they should be excluded from our model.



```{r Model training}

# PLUG IN MODEL
regData <- d3n # Update with tested model

# Remove the toPredict homes
regData <- regData %>%
  filter(toPredict != 1)

# Split data into training (75%) and validation (25%) sets
inTrain <- createDataPartition(
  y = paste(
    regData$extWall,
    regData$floodRisk,
    regData$neighborhood
  ),
  p = 0.75, list = FALSE)

homes.training <- regData[inTrain,]
homes.test <- regData[-inTrain,]

# Estimate model on training set
reg.training <- lm(logPrice ~ .,
                   data = st_drop_geometry(regData) %>%
                     dplyr::select(-toPredict, -MUSA_ID, -price)
)


# Calculate MAE and MAPE
homes.test <- homes.test %>%
  mutate(
    Regression = "Neighborhood effect",
    logPrice.Predict = (predict(reg.training, homes.test)),
    price.Predict = exp(logPrice.Predict),
    price.Error = price.Predict - price,
    price.AbsError = abs(price.Predict - price),
    price.APE = (abs(price.Predict - price)/price.Predict)    
  )

mean(homes.test$price.AbsError) # MAE
mean(homes.test$price.APE)      # MAPE

# Plot distribution of prediction errors

# TODO replace: hist(homes.test$price.Error, breaks = 50) 
# TODO replace: hist(homes.test$price.AbsError, breaks = 50)
# TODO replace: hist(homes.test$price.APE, breaks = 50)


```





```{r k fold cross validation}

# Perform k-fold cross-validation using caret package
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)


# k-fold model training
reg.cv <- 
  train(
    logPrice ~ .,
    data = st_drop_geometry(homes.training) %>%
      dplyr::select(-toPredict, -MUSA_ID, -price),
    method = "lm", 
    trControl = fitControl, 
    na.action = na.omit
  )
reg.cv

# Plot distribution of MAE
allMAE <- reg.cv$resample[,3]



# TODO: replace. hist(allMAE, breaks = 50)

```

On top of that, we test our model in various ways to check whether it predicts significantly better in some places than in others, by looking for errors clustered in space. The first test, called _spatial lag_, averages the price error of each sample with its neighbors. The second one, _Moran’s I_, is used to prove that there is an underlying spatial structure that clusters together similar prices, instead of prices just being randomly distributed. Finally, we check for _neighborhood effects_, comparing the amount of error if neighborhoods are accounted for or not.

```{r Spatial lag test}

# 5. SPATIAL LAG

coords.test <-  st_coordinates(homes.test) 
# Take value of neighbors for weighted matrix
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
# Create spatial weight matrix
spatialWeights.test <- nb2listw(neighborList.test, style="W")

homes.test %>%
  mutate(lagPriceError = lag.listw(spatialWeights.test, price.Error)) %>%
  ggplot(aes(x=lagPriceError, y=price.Error)) # This doesn't plot anything

# Run the actual Moran's I
moranTest <- moran.mc(homes.test$price.Error, 
                      spatialWeights.test, nsim = 999)


# Plot of the Observed Moran's I and the permutations Moran's I distribution
ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()


left_join(
  st_drop_geometry(homes.test) %>%
    group_by(neighborhood) %>%
    summarize(meanPrice = mean(price, na.rm = T)),
  mutate(homes.test, predict.fe = 
           predict(lm(price ~ neighborhood, data = homes.test), 
                   homes.test)) %>%
    st_drop_geometry %>%
    group_by(neighborhood) %>%
    summarize(meanPrediction = mean(predict.fe))) %>%
  kable() %>% kable_styling()

```


```{r Neighborhood effects}

# 6. NEIGHBORHOOD EFFECTS
# Calculate a baseline regression without the neighborhoods

reg.nhood <- lm(logPrice ~ ., data = as.data.frame(st_drop_geometry(homes.training)) %>% 
                  dplyr::select(-toPredict, -MUSA_ID, -price, -neighborhood))
summary(reg.nhood)


st_drop_geometry(regData)
homes.test.nhood <-
  homes.test %>%
  mutate(Regression = "Baseline regression",
         logPrice.Predict = predict(reg.nhood, homes.test), 
         price.Predict = exp(logPrice.Predict),
         price.Error = price.Predict- price,
         price.AbsError = abs(price.Predict- price),
         price.APE = (abs(price.Predict- price)) / price) 

mean(homes.test.nhood$price.AbsError) # MAE
mean(homes.test.nhood$price.APE)      # MAPE


bothRegressions <- rbind(
  dplyr::select(homes.test, starts_with("price"), Regression, neighborhood) %>%
  mutate(lagPriceError = lag.listw(spatialWeights.test, price.Error)),
  dplyr::select(homes.test.nhood, starts_with("price"), Regression, neighborhood) %>%
  mutate(lagPriceError = lag.listw(spatialWeights.test, price.Error))
  )

# Produce a table comparing non-neighborhood effects of a 'Baseline' regression
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -neighborhood) %>%
  filter(Variable == "price.AbsError" | Variable == "price.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable() %>% kable_styling()


bothRegressions %>%
  dplyr::select(price.Predict, price, Regression) %>%
    ggplot(aes(price, price.Predict)) +
  geom_point() +
  stat_smooth(aes(price, price), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(price.Predict, price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme()


# set neighborhood geometry
nhoods <- tracts %>%
  rename(neighborhood = geounit[idx])

st_drop_geometry(bothRegressions) %>%
  group_by(Regression, neighborhood) %>%
  summarize(mean.MAPE = mean(price.APE, na.rm = T)) %>%
  ungroup()%>%
  left_join(nhoods) %>%
  st_sf() %>%
  ggplot() + 
    geom_sf(aes(fill = mean.MAPE)) +
    geom_sf(data = bothRegressions, colour = "black", size = .5) +
    facet_wrap(~Regression) +
    scale_fill_gradient(low = palette5[1], high = palette5[5],
                        name = "MAPE") +
    labs(title = "Mean test set MAPE by neighborhood") +
    labs(title = "Census tracts") +
    mapTheme()


```


Finally, we test the generalizability of our model by comparing the distribution of the errors left in the model with the income context, namely, a map of the neighborhoods where most people have an income below the county median versus those where most people have an income above it.



```{r Generalizability Test}

# 7. GENERALIZABILITY TEST

# Replace with NEW INCOME DATA
incomeTest <- blockGroups %>%
  select(
         pctIncome000,
         pctIncome010,
         pctIncome015,
         pctIncome020,
         pctIncome025,
         pctIncome030,
         pctIncome035,
         pctIncome040,
         pctIncome045,
         pctIncome050,
         pctIncome060,
         pctIncome075,
         pctIncome100,
         pctIncome125,
         pctIncome150,
         pctIncome200
         ) %>%
  mutate(lowIncome =
           pctIncome000+
           pctIncome010+
           pctIncome015+
           pctIncome020+
           pctIncome025+
           pctIncome030+
           pctIncome035+
           pctIncome040+
           pctIncome045+
           pctIncome050+
           pctIncome060+
           pctIncome075,
         highIncome=
           pctIncome100,
           pctIncome125,
           pctIncome150,
           pctIncome200) %>%
  mutate(incomeContext = ifelse((lowIncome) > .5, "Low Income", "High Income")) %>%
  select(-starts_with('pctIncome'), -lowIncome, -highIncome)


# Plot the income groups division
grid.arrange(ncol = 1,
  ggplot() + geom_sf(data = na.omit(incomeTest), aes(fill = incomeContext)) +
    scale_fill_manual(values = c(palette5[4], palette5[3], palette5[2]), name="Income Context") +
    #scale_discrete(limits=dat$V1) +
    labs(title = "Income Context") +
    mapTheme() + theme(legend.position="bottom"))


st_join(bothRegressions, incomeTest) %>% 
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by household income context") %>% kable_styling()


```
***

#### 4. Results



> Provide a polished table of your (training set) lm summary results (coefficients, R2, etc).


```{r summary statistics, results = "asis"}

stargazer(reg.training, type = "html")


```




> Provide a polished table of mean absolute error and MAPE for a single test set. Check out the “kable” function for markdown to create nice tables.

> Provide the results of your cross-validation tests. This includes mean and standard deviation MAE. Do 100 folds and plot your cross-validation MAE as a histogram. Is your model generalizable to new data?

> Plot predicted prices as a function of observed prices

> Provide a map of your residuals for your test set. Include a Moran’s I test and a plot of the spatial lag in errors.

> Provide a map of your predicted values for where ‘toPredict’ is both 0 and 1.

> Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighborhood.

> Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.

> Using tidycensus, split your city into two groups (perhaps by race or income) and test your model’s generalizability. Is your model generalizable?

<<<<<<<<<<<<<<<<<<<


***

#### 5. Discussion





We consider this to be an effective model because by just using open, easy to access data, it can predict home prices in Boulder County with a confidence of _____ . 

>>>>What were some of the most interesting variables?  [ADD A THIRD ONE?]<<<<

In the hope of assessing if the increasing effects of climate change have had any effects on the prices of houses in Boulder, we developed two natural disaster-based evaluation variables from data in Boulder County’s Open Data website.

First, we looked into Floodplains Hazard data. In 2013, Boulder was hit by a flood event that destroyed 2000 homes and left [at least $4 billion in damage](https://www.bouldercast.com/the-2013-boulder-flood-two-years-and-three-billion-dollars-later/). This catastrophe led the state to create the [Colorado Hazard Mapping Program (CHAMP)](https://coloradohazardmapping.com/champ), in order to produce more precise flood hazard prediction models, of which the data we incorporated is extracted.

Additionally, we seeked to assess the risk of exposure to the [increasingly frequent wildfires in Colorado](https://www.nbcnews.com/news/us-news/3-largest-wildfires-colorado-history-have-occurred-2020-n1244525), using the historical wildfire perimeters in the County, which could be controlled not only by the distance of houses from the combined wildfire extensions, but also by how far back in time events are taken into account (10-, 15- or 20-years ago). We did so by counting the number of wildfire events that have occured from a [two-mile distance](https://www.researchgate.net/publication/222788889_Do_Nearby_Forest_Fires_Cause_A_Reduction_in_Residential_Property_Values) from a home.

> [map that shows wildfire data] <

> How much of the variation in prices could you predict? <

R squared, Abs error, mAPE.???

> Describe the more important features? <

House quality, pct income group? HHvacancy-why?. 

> Describe the error in your predictions? <

> Where is it, how is it distributed in price and space? <

>[2x2 maps showing errors in the baseline model vs. errors with ad hoc ‘subcommunities’ as neighborhoods, vs. ‘census tracts’ as neighborhoods]<

Boulder’s Open Data web database does not provide off-the-shelf county-wide neighborhood boundaries, so in order to look into the spatial variation in prices county-wide, we created our own ‘neighborhoods’ by joining a Boulder city ‘subcommunities’ map into the city zoning map and then merging it with the county wide municipalities and unincorporated areas map.

Because of the ad-hoc nature of these boundaries, we tested how well they performed versus the ‘census tracts as neighborhoods’ approach, acknowledging that doing so introduces a certain amount of scale bias into the model. In the end, the census tracts proved to be more effective on a county-wide level because they better differentiate between areas that in our ‘neighborhoods’ were all encompassed as “agricultural” or “forest” areas. 

Since we chose census tracts as neighborhoods, we tried to reduce redundancies and spatial biases (or [MAUP](https://en.wikipedia.org/wiki/Modifiable_areal_unit_problem)) by aggregating the census data on the more granular block group level.

>Where did the model predict particularly well? Poorly? Why do you think this might be?<



***

#### 6. Conclusion





We would confidently recommend our model to Zillow, but we would make sure to note its limitations. For reasons mentioned before, we consider that a model trained on data from an area as homogeneous and arbitrarily defined as Boulder County is going to inevitably bear a great amount of selection and scale bias that may make it unreliable and not generalizable in more demographically diverse geographies.

Nonetheless, we believe our model has a smart approach at selecting and transforming the right internal and external characteristics and services that influence house values, and on which it could be easy to build on to better predict on a more diverse study area. For example, an optimized version of our model could instead be trained on aggregate data from the ten counties that form the **Denver-Aurora-Lakewood Combined Statistical Area**, of which Boulder is a part of.

Additionally, the model can greatly benefit from applying other more powerful and data-specific methods of predictions that we did not implement at the moment.
